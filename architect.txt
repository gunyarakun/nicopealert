* ニコペアラート

ニコニコ大百科/ニコニコ生放送で起きる各種イベントを通知する。

** ねらい

- ニコニコ大百科のPV数増加。
- 俺が便利
- ニコ生ユーザにも便利に作りこむことによって、ニコニコ大百科への自然な誘導を果たす。

** 設計

*** 基本設計

+ ログインはしない。
+ ニコニコ大百科/ニコニコ生放送からイベントを取得する。
+ ユーザが指定したフィルタ群に通し、通ったものだけを通知。

以上。

この文書が指摘しておきたいのは、あくまでイベントは「全ユーザ共通」であるということ。
サーバ側でフィルタしていたらスケールしない。
あくまでサーバ側ではstaticなfeedを出すべき。
ログインは要求しない。

*** 基本設計の問題点

ニコニコ生放送ではフィードを出していない。
ユーザで各htmlをクロールすることも考えられるが、
MOTTAINAI!!!!
というわけで、ニコニコ大百科のサーバでクロールを行い、
そのデータをfeedする。

*** 基本設計の詳細化

feedの内容はどうすべきか。ニコニコ大百科とニコニコ生放送のイベントを分けるべきか。
基本的
・イベントの対象URL
・イベントの種類
の２つだけでOKではないか。

本当にOKかどうか、通知したいイベントの一覧を挙げてみる。
それらが上記のフレームワークにあてはまればOK。

*** 通知したいイベントの一覧

基本的には、「ユーザが何がしかのデータをデータベースに永続化した場合」を考えるとよいだろう。

- ニコニコ生放送
-- 放送開始
-- 放送終了

- ニコニコ大百科
-- 新規記事の登場
-- 記事の編集
-- 掲示板のレス
--- お絵カキコレス
--- ピコカキコレス
-- ほめる

これらのうち、不必要なイベントとは何か。また、実装不能なイベントとは何か。

- ニコニコ生放送
-- 放送開始 URL・生主・タイトル・


*** ニコ生での例外について

枠予約についてはどうか。枠予約については、生放送が始まってしまえば一般枠と同じ扱いでよい（設計当時は）。「予約枠と思われる部分のhtmlの解析はしない」で現状では十分。

枠中断についてはどうか。枠中断については、「通知したい生放送」がなくなることを意味する。しかし、ユーザとしてはすでにその生放送が見れないわけで、それを通知する必要もない。

枠延長についてはどうか。アラートを立ち上げた時点で生放送を行っているのであれば、ユーザとしてはその生放送を通知しておいて欲しいだろう。

というわけで、「現在生放送している一覧」をニコ百サーバでホストしなければいけない。
これは、ニコ百サーバが出すfeedの要件となる。

*** ニコ百サーバが出すfeedの仕様

feedの仕様はどうすべきか。たとえば、rssやatomなどを用いるべきか。
用いるべきではないと考える。
第１に、rss/atomは冗長である。
第２に、互換性を取ったからといって実装の難易度が下がるわけではない。
独自形式のfeedにすべきである。

では、どのような形式で構造化を行うべきか。
- XML
- JSON
- YAML
JSONはYAMLのsubsetである。YAMLほどの自由度は必要とされない。
XMLかJSONかで判断すると、jsonの方が俺好みだし、parserも早いはず。
ただし、jsonが他のニコ生ツールにタダ乗りされてしまう可能性がある。
しかし、XMLでも状況は同じわけで、特にタダ乗りのリスクを挙げるほどではないだろう。
jsonのほうが全体のデータサイズも抑えられる。

というわけで、JSONを採用することとする。

JSONは具体的にどのような形式にすべきか？
基本的には、eventのarrayがあって、それをクロールするという形になる。
リモートでのポーリングで確実なQueueを実装するのは面倒。
現状のrss/atomリーダ的なfetchで十分だろう。
問題点としては、fetch間隔内でfeedの最大イベント個数を超えたイベントが起こった場合である。
この場合、取りこぼしが発生する。取りこぼしはユーザ満足度を著しく下げる。
なぜなら、ユーザは「取りこぼしをしたくない」からこのようなツールを導入するからである。

ニコニコ大百科のイベントは毎分起こるわけではないか、ニコニコ生放送のイベントは毎分起こる。
定期/非常メンテナンス時はまったくイベントがおこらない。
ニコニコ生放送は、メンテナンス後いきなり多くの生放送が始まる。

生放送の最大枠などの事前情報を用いて、あるfetch間隔内での最大イベント数を決めることが可能かもしれない。
しかし、本質的には「ある時間内に生じるイベントの数は未確定」であろう。

よって、ある時間内に起こったイベントをすべて記述したファイルが必要となる。
具体的には、１分ごとに起こったイベントを記録したファイルを用意する。
このファイルには、その１分間に起こったイベントすべてが記録される。
また、ファイルは存在する場合、すべてのイベントを含むものとする。
よって、１分のうち0秒に起こったイベントは、60秒後にしか知りえなくなる。

果たして、この設計でいいのか？
現状のアラートプログラムの不満として、「１分経たないと生を検地できない」というものがある。
１分経たないといけないのは、ニコ生側のプログラムの制約か、それともアラート側の制約か見極める必要がある。

１分経たないとニコ生一覧に生放送が出ないとしよう。その場合は、ニコ生一覧ページをクロールした瞬間に、昔発生しているはずのイベントが生じるということになってします。
後者の場合は、特に問題ない。
僕としては、前者の場合でも安全なように設計をしておきたい。そのために、有効なクロールポリシーは存在するか？

*** ニコ生クロール

ニコニコ生放送には、lvxxxxxという通し番号が付与されている。通し番号を用いればすべての生放送が理論上クロールできる。ただし、問題として、
- 枠予約の場合は、lvxxxxの番号と時系列とが矛盾する
- 生放送をとったけど放送しなかった、などの場合に空きが生ずる
などの問題で、通し番号を用いたクロールは困難であると判断する。

*** ニコ百サーバにsocket通信する

却下。ポート足りねっす。

*** udp broadcast

却下。届かない可能性あり。

** むむむ

どうしよう。こういう場合は、楽観的にいくのがよい。
- JSON形式
- 1つのファイルには、ある1分間に発生したイベントが書かれる。ファイルが書かれるのは、その1分間が終了した瞬間である。

むむむ…分が変わった瞬間というのは、サーバ側とユーザ側の時計のズレで異なってしまう。よって最大２分の遅れが出てしまう。
非常によろしくない。

ん…待てよ、固定イベント数でファイルを区切るといいんじゃないか？
どうせ独自形式なんだから、どういう風にファイルを定義するのも自由である。
- JSON形式
- 1つのファイルには、イベントが1000個書かれる。
- 1000個書かれたら、次のファイルに移動。
- ファイル名は連番。
- クライアントは、HTTPのrange指定をして一部だけfetchをすることが望まれる。

テラ2ch式。ファイルのidがunix timeじゃなくて連番であり、subject.txtの手助けがいらない部分は異なるけどね。
俺が生きている間くらいは回るな。これでOK。
クライアントはがんばってファイルをずっと読んでいけばいい…
んー待てよ、クライアントの起動直後はどうするよ？連番の最初を知る方法が必要だー。
というわけで、やはりsubject.txt的なインデックスは必要そうだ。
むー。なんとか、クライアント側の現在時刻を基にして一発で参照すべきファイルが分かるといいんだけどなー。

時刻+連番はどうか。
ある分について、最大1000個のイベントしか１ファイルに書かれない。
1000個を超えるイベントは、別ファイルとして連番に書かれる。

んー…

そもそも、最初の「分刻みのファイル」で、クライアント側が追記をずっと読んでいけばいいんじゃないの？
うー、それでよさそうだ。

** ニコ百サーバが出すfeedの形式（荒）

- ファイル名: yyyymmddhhmm.feed
- 内容: json、 [{"url":"target", "key": value}]、urlによって、keyの一覧や解釈を変えることとする。
おおう。すっきり。でも、jsonのarrayと、httpのrangeって相性悪そう…
ここは、jsonすべてを取得してもらうか。そっちのほうがすっきりするだろ。
いやー…改行区切りで、各行にjsonのhashがあったほうがいいんじゃないか？

悩む…
